"""Fallback data source: HTML scraping with BeautifulSoup.

Only used when the TSV download fails. Scrapes the Netflix Top 10
website directly, one page per country-category combination.

Why data-uia selectors instead of CSS classes:
    The previous scraper (main.py) used CSS classes like "css-1rheyty"
    which are auto-generated by Netflix's CSS-in-JS toolchain and change
    on every site redesign. The data-uia attributes (e.g.
    "top10-table-row-title") are Netflix's own test automation hooks -
    they're semantic, stable, and unlikely to change.

Rate limiting:
    1.5s delay between requests to avoid overwhelming Netflix's servers.
    For 18 countries x 2 categories = 36 requests, this takes ~54 seconds.
"""

from __future__ import annotations

import logging
import re
import time

import requests
from bs4 import BeautifulSoup

from src.config import TRACKED_COUNTRIES, NetflixConfig
from src.models import CountryRanking, RankingEntry

logger = logging.getLogger(__name__)

CATEGORIES = ("", "tv")


def fetch_all_countries(
    session: requests.Session,
    config: NetflixConfig,
) -> tuple[CountryRanking, ...]:
    """Scrape Top 10 rankings for all tracked countries.

    Iterates through every country in TRACKED_COUNTRIES and both
    categories (films, tv), fetching each page individually with a
    delay between requests. Errors on individual pages are logged
    but don't stop the overall process.

    Args:
        session: HTTP session with retry strategy.
        config: Netflix configuration with base_url and html_delay.

    Returns:
        Tuple of successfully scraped CountryRanking objects.
        May be shorter than expected if individual pages fail.
    """
    results: list[CountryRanking] = []
    errors: list[str] = []

    for country_name, country_slug in sorted(TRACKED_COUNTRIES.items()):
        for category_suffix in CATEGORIES:
            category_label = "tv" if category_suffix == "tv" else "films"
            try:
                ranking = _fetch_single_page(
                    session, config, country_name, country_slug,
                    category_suffix, category_label,
                )
                if ranking is not None:
                    results.append(ranking)
            except (requests.RequestException, ValueError) as exc:
                msg = (
                    f"Failed to fetch {country_name}/{category_label}: {exc}"
                )
                logger.error(msg)
                errors.append(msg)

            time.sleep(config.html_delay)

    if errors:
        logger.warning("HTML fetch completed with %d errors", len(errors))

    logger.info("HTML fetcher returned %d rankings", len(results))
    return tuple(results)


def _fetch_single_page(
    session: requests.Session,
    config: NetflixConfig,
    country_name: str,
    country_slug: str,
    category_suffix: str,
    category_label: str,
) -> CountryRanking | None:
    """Fetch and parse a single country/category page.

    Builds the URL (e.g. netflix.com/tudum/top10/south-korea/tv),
    downloads the HTML, and extracts the week identifier and
    ranking entries using data-uia attribute selectors.

    Args:
        session: HTTP session with retry strategy.
        config: Netflix configuration with base_url and timeout.
        country_name: Human-readable name (e.g. "South Korea").
        country_slug: URL slug (e.g. "south-korea").
        category_suffix: "" for films, "tv" for TV shows.
        category_label: "films" or "tv" for the data model.

    Returns:
        A CountryRanking object, or None if no entries were found.

    Raises:
        requests.RequestException: On HTTP errors.
    """
    path = f"{config.base_url}/{country_slug}"
    if category_suffix:
        path = f"{path}/{category_suffix}"

    logger.debug("Fetching %s", path)
    response = session.get(path, timeout=config.request_timeout)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "lxml")
    week = _extract_week(soup)
    entries = _extract_rankings(soup)

    if not entries:
        logger.warning("No entries found for %s/%s", country_name, category_label)
        return None

    return CountryRanking(
        week=week,
        country=country_slug,
        country_name=country_name,
        category=category_label,
        source="html_fallback",
        rankings=entries,
    )


def _extract_week(soup: BeautifulSoup) -> str:
    """Extract the week date from the page's week selector dropdown.

    Netflix displays the week as a date range like "1/26/26 - 2/1/26"
    in a custom dropdown. We extract the end date and convert it to
    ISO format (YYYY-MM-DD) to match the TSV format.

    Two extraction strategies (tried in order):
    1. Look for the "selected" display text in the combobox
    2. Look for the option element with "selected" CSS class

    Args:
        soup: Parsed HTML of a Netflix Top 10 page.

    Returns:
        ISO date string (e.g. "2026-02-01"), or "unknown" if extraction fails.
    """
    week_select = soup.find(
        attrs={"data-uia": "top10-filters-week-select"}
    )
    if week_select:
        selected = week_select.find(
            attrs={"data-uia": "top10-filters-week-select-selected"}
        )
        if selected:
            text = selected.get_text(strip=True)
            match = re.search(
                r"(\d{1,2})/(\d{1,2})/(\d{2,4})", text
            )
            if match:
                month, day, year = match.groups()
                if len(year) == 2:
                    year = f"20{year}"
                return f"{year}-{int(month):02d}-{int(day):02d}"

    option = soup.find(
        attrs={"data-uia": "top10-filters-week-select-option"},
        class_=re.compile(r"selected"),
    )
    if option:
        text = option.get_text(strip=True)
        parts = text.split(" - ")
        if len(parts) == 2:
            match = re.search(
                r"(\d{1,2})/(\d{1,2})/(\d{2,4})", parts[1]
            )
            if match:
                month, day, year = match.groups()
                if len(year) == 2:
                    year = f"20{year}"
                return f"{year}-{int(month):02d}-{int(day):02d}"

    return "unknown"


def _extract_rankings(soup: BeautifulSoup) -> tuple[RankingEntry, ...]:
    """Extract the Top 10 ranking entries from the page's table.

    Uses data-uia attributes to find table cells:
    - "top10-table-row-title": contains rank number (in a <span class="rank">)
      and title (in a <button> element)
    - "top10-table-row-weeks": contains cumulative weeks in Top 10

    Falls back to positional index (idx + 1) if the rank <span> is
    missing, and to the last stripped string if the <button> is missing.

    Args:
        soup: Parsed HTML of a Netflix Top 10 page.

    Returns:
        Tuple of up to 10 RankingEntry objects, or empty tuple if no
        table data found.
    """
    title_cells = soup.find_all(
        "td", attrs={"data-uia": "top10-table-row-title"}
    )
    weeks_cells = soup.find_all(
        "td", attrs={"data-uia": "top10-table-row-weeks"}
    )

    if not title_cells:
        return ()

    entries = []
    for idx, title_cell in enumerate(title_cells):
        rank_span = title_cell.find("span", class_="rank")
        rank = idx + 1
        if rank_span:
            rank_text = rank_span.get_text(strip=True)
            try:
                rank = int(rank_text)
            except ValueError:
                pass

        button = title_cell.find("button")
        if button:
            title = button.get_text(strip=True)
        else:
            texts = list(title_cell.stripped_strings)
            title = texts[-1] if texts else ""

        weeks_in_top_10 = 0
        if idx < len(weeks_cells):
            try:
                weeks_in_top_10 = int(
                    weeks_cells[idx].get_text(strip=True)
                )
            except ValueError:
                pass

        entries.append(
            RankingEntry(
                rank=rank,
                title=title,
                weeks_in_top_10=weeks_in_top_10,
                hours_viewed=0,
            )
        )

    return tuple(entries)
